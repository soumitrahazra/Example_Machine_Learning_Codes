{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSE3QnOqn299D+jq0USQuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumitrahazra/Example_Machine_Learning_Codes/blob/main/NLP_Introductory_Analysis1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot go straight from raw text to fitting a machine learning or deep learning model. We must clean our text first, which means splitting it into words and handling punctuation and case. In fact, there is a whole suite of text preparation methods that we may need to use, and the choice of methods really depends on our natural language processing task."
      ],
      "metadata": {
        "id": "DTrlYguV697-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6sE3RWq06Pkp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsF2_hGS7-og",
        "outputId": "3ba9b49b-e782-4448-cf58-6e8994f67752"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # load the dataset, split into input (X) and output (y) variables\n",
        " filename='/content/drive/MyDrive/metamorphosis_clean.txt'"
      ],
      "metadata": {
        "id": "u9k1JBXf8UnW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(filename, 'rt')"
      ],
      "metadata": {
        "id": "hXn4k_t08a_r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = file.read()\n",
        "file.close ()\n"
      ],
      "metadata": {
        "id": "ELREERh_9JvA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words by white space\n",
        "words = text.split()\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP2_YR2L9XAT",
        "outputId": "d4352752-cfb9-4fc7-e21f-5b8264528c14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split based on words only\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJtNUP6Z9fSs",
        "outputId": "453d5f9c-4f4e-4998-b655-45771141da3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words by white space words = text.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZS0aLFS-Bno",
        "outputId": "fbc38ccc-e31b-481c-c68f-e00b300b2a07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sometimes text data may contain non-printable characters. We can use a similar approach to filter out all non-printable characters.\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "result = [re_print.sub('', w) for w in words]\n",
        "print(result[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VVx9UtG-nH4",
        "outputId": "8543ecec-654e-4f56-c484-05cb9635d3d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poUdU_10ATga",
        "outputId": "570162d9-2486-4fdb-f731-ac8957a5e37b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'what', 's', 'happened', 'to', 'me', 'he', 'thought', 'it', 'wasn', 't', 'a', 'dream', 'his', 'room']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JElYILNAAt8_",
        "outputId": "30259ed0-9fce-4155-8db0-bc82690b05f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-H0wfl2DJeo",
        "outputId": "93f77368-3c4c-429f-dc42-55f09a77ad59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# load data\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close ()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3swtJ0KxCCLZ",
        "outputId": "04233fc4-313d-4de4-f495-464afe996871"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '“', 'What', '’', 's', 'happened']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu_Jcm1XlSQC",
        "outputId": "b843bf11-e63b-4094-c8dd-9742bf8b4844"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "w_mcNQhAebvt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset, split into input (X) and output (y) variables\n",
        "filename='/content/drive/MyDrive/metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close ()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]"
      ],
      "metadata": {
        "id": "ojMSkLwDknX_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can filter out all tokens that we are not interested in, such as all standalone punctuation. This can be done by iterating over all tokens and only keeping those tokens that are all alphabetic. Python has the function isalpha() that can be used.\n",
        "\n",
        "Stop words are those words that do not contribute to the deeper meaning of the phrase. They are the most common words such as: the, a, and is. For some applications like documentation classification, it may make sense to remove stop words. NLTK provides a list of commonly agreed upon stop words for a variety of languages, such as English. They can be loaded as follows:"
      ],
      "metadata": {
        "id": "xIUjmUxBlpFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E8VG-Dfk9YD",
        "outputId": "119e9aaa-e9ed-47e8-d6a6-a99c0bb75d75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'travelling', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'gregor', 'turned', 'look', 'window']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer well defined fixed-length inputs and outputs. Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Specifically, vectors of numbers.\n",
        "\n",
        "In language processing, the vectors x are derived from textual data, in order to reflect various linguistic properties of the text.\n",
        "— Page 65, Neural Network Methods in Natural Language Processing, 2017.\n",
        "\n",
        "This is called feature extraction or feature encoding. A popular and simple method of feature extraction with text data is called the bag-of-words model of text.\n"
      ],
      "metadata": {
        "id": "rwP5lSsHoxHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Bag-of-Words?\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms. The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "\n",
        "• A vocabulary of known words.\n",
        "\n",
        "• A measure of the presence of known words.\n",
        "\n",
        "It is called a bag-of-words , because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
        "\n",
        "A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature.\n",
        "\n",
        "— Page 69, Neural Network Methods in Natural Language Processing, 2017.\n",
        "\n",
        "The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document."
      ],
      "metadata": {
        "id": "0eMYkaF3qdkS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjjCc40rqjxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}